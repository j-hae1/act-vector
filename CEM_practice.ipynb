{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j-hae1/act-vector/blob/main/CEM_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-gcGPBoy4K"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D679oMEoYsG",
        "outputId": "3d8df7b6-2d8d-4258-8832-c56d5f3140a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379367 sha256=86902e8ee741823a4e1104e8d35d2ff087750ff83534e5434b1ddc804c0a1889\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, jedi\n",
            "Successfully installed box2d-py-2.3.5 jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\" pygame matplotlib ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "16k8nmQuod_P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from IPython.display import HTML, Javascript, display\n",
        "\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7peCCmyno2Ys"
      },
      "source": [
        "# CEM algorithm implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p1aG6oDEogUo"
      },
      "outputs": [],
      "source": [
        "class CEM:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        bounds: np.ndarray,\n",
        "        population_size: int = 500,\n",
        "        elite_frac: float = 0.1,\n",
        "        max_iters: int = 50,\n",
        "        smoothing: float = 0.7,\n",
        "        seed: int = None,\n",
        "    ):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.pop_size = population_size\n",
        "        self.n_elite = max(1, int(np.ceil(elite_frac * population_size)))\n",
        "        self.max_iters = max_iters\n",
        "        self.alpha = smoothing\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.best_obj_history = []\n",
        "\n",
        "    def optimize(self, x_init: np.ndarray, objective):\n",
        "        mu = x_init.copy()\n",
        "        sigma = (self.bounds[1] - self.bounds[0]) / 2.0\n",
        "\n",
        "        self.best_obj_history = []\n",
        "\n",
        "        for i in range(self.max_iters):\n",
        "            pass\n",
        "\n",
        "            # 1. random sample x and clip with bounds\n",
        "            samples = self.rng.normal(loc=mu, scale=sigma, size=(self.pop_size, self.dim))\n",
        "            samples = np.clip(samples, self.bounds[0], self.bounds[1])\n",
        "\n",
        "            # 2. query y values from sampled x\n",
        "            ys = np.array([objective(sample) for sample in samples])\n",
        "\n",
        "            # 3. get top k minimum (x,y)\n",
        "            sorted_indices = np.argsort(ys)\n",
        "            elite_idxs = sorted_indices[:self.n_elite]\n",
        "            elite_samples = samples[elite_idxs]\n",
        "\n",
        "            # 3. calculate mean, std from top k samples\n",
        "            new_mu = np.mean(elite_samples, axis=0)\n",
        "            new_sigma = np.std(elite_samples, axis=0) + 1e-6\n",
        "\n",
        "            # 3. update mean and sigma\n",
        "            mu = self.alpha * new_mu + (1 - self.alpha) * mu\n",
        "            sigma = self.alpha * new_sigma + (1 - self.alpha) * sigma\n",
        "\n",
        "            # [optional] For debug, log best y values and mean y values, etc ..\n",
        "            best_y = ys[elite_idxs[0]]\n",
        "            mean_y = ys.mean()\n",
        "            self.best_obj_history.append(best_y)\n",
        "            print(f\"Iteration: {i} / best_y: {best_y} / mean_y: {mean_y}\")\n",
        "\n",
        "        # return final mean\n",
        "        return mu\n",
        "\n",
        "    def visualize(self):\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(1, len(self.best_obj_history) + 1), self.best_obj_history, marker='o')\n",
        "        plt.title(\"Best Value per Iteration\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Objective (Lower is Better)\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqGqMM-n2aP"
      },
      "source": [
        "# Objective function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qs9SiMezp_8d"
      },
      "outputs": [],
      "source": [
        "def objective_vec(train_env, batch: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generic vectorized discrete-action objective for CEM (to be MINIMIZED).\n",
        "    Works for CartPole (action_dim=2) or LunarLander (action_dim=4) or any other discrete gym env.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_env : VectorEnv\n",
        "        A vectorized gymnasium environment.\n",
        "    batch : np.ndarray, shape (N, D)\n",
        "        Population of parameter vectors. Each θ has length\n",
        "            obs_dim * action_dim  +  action_dim\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray, shape (N,)\n",
        "        Negative average reward per candidate (CEM will minimize).\n",
        "    \"\"\"\n",
        "    N, D = batch.shape\n",
        "    obs_dim = train_env.single_observation_space.shape[0]\n",
        "    action_dim = train_env.single_action_space.n\n",
        "\n",
        "    # sanity check\n",
        "    assert D == obs_dim * action_dim + action_dim, (\n",
        "        f\"Expected θ to be length {obs_dim*action_dim + action_dim}, \"\n",
        "        f\"got {D}\"\n",
        "    )\n",
        "\n",
        "    results = np.zeros(N, dtype=float)\n",
        "\n",
        "    for i in range(N):\n",
        "        theta = batch[i]\n",
        "        # 1. unpack θ → W, b\n",
        "        W = theta[: obs_dim * action_dim].reshape(obs_dim, action_dim)\n",
        "        b = theta[obs_dim * action_dim :]\n",
        "\n",
        "        # 2. get sum of rewards with policy and env\n",
        "        # Note: sum of rewards should be maximized, but CEM works to minimize objective. So we should change the sign.\n",
        "        obs, _ = train_env.reset()\n",
        "        done_mask = np.zeros(train_env.num_envs, dtype=bool)\n",
        "        cum_rewards = np.zeros(train_env.num_envs, dtype=float)\n",
        "\n",
        "        # run until each sub-env ends\n",
        "        while not done_mask.all():\n",
        "            # compute per‐env logits and pick argmax\n",
        "            logits = obs.dot(W) + b        # (num_envs, action_dim)\n",
        "            actions = np.argmax(logits, axis=1)\n",
        "\n",
        "            obs, rewards, terminated, truncated, _ = train_env.step(actions)\n",
        "            active = ~done_mask\n",
        "            cum_rewards[active] += rewards[active]\n",
        "            done_mask |= (terminated | truncated)\n",
        "\n",
        "        # store –CEM minimizes, so negate average reward\n",
        "        results[i] = -cum_rewards.mean()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwyBY8Gpn2aR"
      },
      "source": [
        "## Helper function - evaluation\n",
        "\n",
        "You don't need to implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1nwwEosfvgCv"
      },
      "outputs": [],
      "source": [
        "def animate_policy(\n",
        "    theta: np.ndarray,\n",
        "    env_name: str = \"CartPole-v1\",\n",
        "    max_steps: int = 500,\n",
        "    interval: int = 100,\n",
        "    goal_reward: float = None\n",
        ") -> HTML:\n",
        "    \"\"\"\n",
        "    Render a discrete-action policy for any Gymnasium env and display a reward curve\n",
        "    with an optional goal_reward marker.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    theta : np.ndarray, shape=(obs_dim*action_dim + action_dim,)\n",
        "        Flattened policy parameters.\n",
        "    env_name : str\n",
        "        Gym environment ID.\n",
        "    max_steps : int\n",
        "        Max timesteps to run.\n",
        "    interval : int\n",
        "        Milliseconds between animation frames.\n",
        "    goal_reward : float or None\n",
        "        If provided, draw a horizontal line at this reward and a vertical line\n",
        "        at the first step where cumulative reward ≥ goal_reward.\n",
        "    \"\"\"\n",
        "    # Setup env\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    assert theta.size == obs_dim*action_dim + action_dim, \\\n",
        "        f\"θ length should be {obs_dim*action_dim + action_dim}, got {theta.size}\"\n",
        "    W = theta[:obs_dim*action_dim].reshape(obs_dim, action_dim)\n",
        "    b = theta[obs_dim*action_dim:]\n",
        "\n",
        "    # Rollout and collect frames & rewards\n",
        "    obs, _ = env.reset()\n",
        "    frames, rewards = [], []\n",
        "    total = 0.0\n",
        "    goal_step = None\n",
        "\n",
        "    for t in range(max_steps):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        logits = obs.dot(W) + b\n",
        "        action = int(np.argmax(logits))\n",
        "        obs, r, terminated, truncated, _ = env.step(action)\n",
        "        total += r\n",
        "        rewards.append(total)\n",
        "        if goal_reward is not None and goal_step is None and total >= goal_reward:\n",
        "            goal_step = t+1\n",
        "        if terminated or truncated:\n",
        "            frames.append(env.render())\n",
        "            rewards.append(total)\n",
        "            break\n",
        "    env.close()\n",
        "\n",
        "    # Encode frames\n",
        "    uris = []\n",
        "    for f in frames:\n",
        "        img = Image.fromarray(f)\n",
        "        buf = io.BytesIO()\n",
        "        img.save(buf, format=\"PNG\")\n",
        "        uris.append(\"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode())\n",
        "\n",
        "    # Plot reward curve with goal markers\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(len(rewards)), rewards, marker='o')\n",
        "    if goal_reward is not None:\n",
        "        ax.axhline(goal_reward, color='red', linestyle='--', label=f\"Goal = {goal_reward}\")\n",
        "        if goal_step is not None:\n",
        "            ax.axvline(goal_step, color='green', linestyle='--',\n",
        "                       label=f\"Reached at t={goal_step}\")\n",
        "        ax.legend()\n",
        "    ax.set_title(\"Cumulative Reward\")\n",
        "    ax.set_xlabel(\"Step\")\n",
        "    ax.set_ylabel(\"Total Reward\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"PNG\")\n",
        "    plt.close(fig)\n",
        "    graph_uri = \"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode()\n",
        "\n",
        "    # Build HTML\n",
        "    total_frames = len(frames)\n",
        "    w = frames[0].shape[1]\n",
        "    h = frames[0].shape[0]\n",
        "\n",
        "    charset = string.ascii_letters + string.digits\n",
        "    chars = np.array(list(charset))\n",
        "    picks = np.random.choice(chars, size=6)\n",
        "    # Join them into one string\n",
        "    frame_id = ''.join(picks)\n",
        "\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <div style=\"text-align:center;\">\n",
        "      <img id=\"cem_frame_{frame_id}\" width=\"{w}\" height=\"{h}\" style=\"border:1px solid #333\"/>\n",
        "      <p id=\"frame_counter_{frame_id}\">Frame: 1 / {total_frames}</p>\n",
        "      <div>\n",
        "        <img src=\"{graph_uri}\" width=\"400\"/>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "    js = f\"\"\"\n",
        "    const frames = [{','.join(f\"'{u}'\" for u in uris)}];\n",
        "    let idx = 0, total = frames.length;\n",
        "    const img = document.getElementById('cem_frame_{frame_id}'),\n",
        "          counter = document.getElementById('frame_counter_{frame_id}');\n",
        "    img.src = frames[0];\n",
        "    setInterval(() => {{\n",
        "      idx = (idx + 1) % total;\n",
        "      img.src = frames[idx];\n",
        "      counter.innerText = 'Frame: ' + (idx+1) + ' / ' + total;\n",
        "    }}, {interval});\n",
        "    \"\"\"\n",
        "    display(Javascript(js))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AfNR4_0qglU"
      },
      "source": [
        "# Optimization example : Cartpole\n",
        "- [Gymnasium docs](https://gymnasium.farama.org/v0.29.0/environments/classic_control/cart_pole/)\n",
        "- Takes about 30 sec ~ 1 min to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "a1DPRX8JogPO",
        "outputId": "63423868-7363-4540-92f5-27c7db7b3a30"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'NoneType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-348037818318>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mcem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCEM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melite_frac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m best_theta = cem.optimize(\n",
            "\u001b[0;32m<ipython-input-3-e4aa876a5f42>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim, bounds, population_size, elite_frac, max_iters, smoothing, seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_elite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melite_frac\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "train_env = gym.make_vec(\n",
        "    \"CartPole-v1\",\n",
        "    num_envs = 3,\n",
        "    vectorization_mode = \"vector_entry_point\"\n",
        ")\n",
        "obs_dim = train_env.single_observation_space.shape[0]\n",
        "action_dim = train_env.single_action_space.n\n",
        "objective = lambda batch: objective_vec(train_env, batch)\n",
        "\n",
        "\"\"\"\n",
        "You need to define\n",
        "1. Dimension of linear policy weight.\n",
        "    NOTE:\n",
        "        Weight * Observation = Action.\n",
        "        Weight would include bias term. So the dimension of weight should be ...\n",
        "2. Bound where we should search policy\n",
        "3. hyperparameters of CEM. population_size, elite_frace, max_iters, smoothing ratio etc...\n",
        "\"\"\"\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# 1. Dimension of linear policy weight.\n",
        "dim = None # int\n",
        "# 2. Bound where we should search policy\n",
        "bounds = None # np.array\n",
        "# 3. hyperparameters of CEM\n",
        "population_size = None # int\n",
        "elite_frac = None # float\n",
        "max_iters = None # int\n",
        "smoothing = None # float\n",
        "x0 = None # np.array\n",
        "\n",
        "cem = CEM(dim=dim, bounds=bounds, population_size=population_size, elite_frac=elite_frac, max_iters=max_iters, smoothing=smoothing)\n",
        "\n",
        "best_theta = cem.optimize(\n",
        "    x0,\n",
        "    objective,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqazt-Idn2aU"
      },
      "source": [
        "Test if your policy is working well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqUubxdqvkTg"
      },
      "outputs": [],
      "source": [
        "animate_policy(best_theta, max_steps=500, interval=100, env_name='CartPole-v1', goal_reward=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Om53i3q9Rw"
      },
      "source": [
        "# Optimization example : Lunar Lander\n",
        "- [Gymnasium docs](https://gymnasium.farama.org/v0.29.0/environments/box2d/lunar_lander/)\n",
        "- Takes about 10 min to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btNYtIWVqPeI"
      },
      "outputs": [],
      "source": [
        "train_env = gym.make_vec(\n",
        "    \"LunarLander-v3\",\n",
        "    num_envs = 3,\n",
        ")\n",
        "\n",
        "obs_dim = train_env.single_observation_space.shape[0]\n",
        "action_dim = train_env.single_action_space.n\n",
        "objective = lambda batch: objective_vec(train_env, batch)\n",
        "\n",
        "\"\"\"\n",
        "You need to define\n",
        "1. Dimension of linear policy weight .\n",
        "    NOTE:\n",
        "        Weight * Observation = Action.\n",
        "        Weight would include bias term. So the dimension of weight should be ...\n",
        "2. Bound where we should search policy\n",
        "3. hyperparameters of CEM. population_size, elite_frace, max_iters, smoothing ratio etc...\n",
        "\"\"\"\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# 1. Dimension of linear policy weight.\n",
        "dim = None # int\n",
        "# 2. Bound where we should search policy\n",
        "bounds = None # np.array\n",
        "# 3. hyperparameters of CEM\n",
        "population_size = None # int\n",
        "elite_frac = None # float\n",
        "max_iters = None # int\n",
        "smoothing = None # float\n",
        "x0 = None # np.array\n",
        "\n",
        "cem = CEM(dim=dim, bounds=bounds, population_size=population_size, elite_frac=elite_frac, max_iters=max_iters, smoothing=smoothing)\n",
        "\n",
        "best_theta = cem.optimize(\n",
        "    x0,\n",
        "    objective,\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n14Wgh1n2aV"
      },
      "source": [
        "Test if your policy is working well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqOuTL3MtnkB"
      },
      "outputs": [],
      "source": [
        "animate_policy(best_theta, max_steps=500, interval=100, env_name='LunarLander-v3', goal_reward=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez3WBBign2aW"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZzg9zURn2aW"
      },
      "outputs": [],
      "source": [
        "!python -m pip install cma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpr08q6RvSmg"
      },
      "outputs": [],
      "source": [
        "# Having fun with CMA-ES\n",
        "import cma\n",
        "\n",
        "es = cma.CMAEvolutionStrategy(dim * [0], 1.0, {'popsize': 100, 'maxiter':200})\n",
        "while not es.stop():\n",
        "  solutions = es.ask()\n",
        "  res = objective_vec(train_env, np.stack(solutions, axis=0))\n",
        "  es.tell(solutions, res)\n",
        "  es.logger.add()  # write data to disc to be plotted\n",
        "  es.disp()\n",
        "es.result\n",
        "animate_policy(es.result.xbest,\n",
        "               max_steps=500, interval=100, env_name='LunarLander-v3', goal_reward=200)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}